{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a5c844",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "Setting environment, including installing necessary libraries and importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf079113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import os\n",
    "import timeit\n",
    "import random\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "import obspy\n",
    "from obspy import Trace, UTCDateTime, read\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "174e05cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc baru\\AppData\\Local\\Temp\\ipykernel_10632\\3882101547.py:6: DtypeWarning: Columns (7,11,13,14,24,25,26,30,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_path)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>network_code</th>\n",
       "      <th>receiver_code</th>\n",
       "      <th>receiver_type</th>\n",
       "      <th>receiver_latitude</th>\n",
       "      <th>receiver_longitude</th>\n",
       "      <th>receiver_elevation_m</th>\n",
       "      <th>p_arrival_sample</th>\n",
       "      <th>p_status</th>\n",
       "      <th>p_weight</th>\n",
       "      <th>p_travel_sec</th>\n",
       "      <th>...</th>\n",
       "      <th>source_magnitude_author</th>\n",
       "      <th>source_mechanism_strike_dip_rake</th>\n",
       "      <th>source_distance_deg</th>\n",
       "      <th>source_distance_km</th>\n",
       "      <th>back_azimuth_deg</th>\n",
       "      <th>snr_db</th>\n",
       "      <th>coda_end_sample</th>\n",
       "      <th>trace_start_time</th>\n",
       "      <th>trace_category</th>\n",
       "      <th>trace_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TA</td>\n",
       "      <td>109C</td>\n",
       "      <td>HH</td>\n",
       "      <td>32.8889</td>\n",
       "      <td>-117.1051</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-10-21 05:55:00</td>\n",
       "      <td>noise</td>\n",
       "      <td>109C.TA_201510210555_NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TA</td>\n",
       "      <td>109C</td>\n",
       "      <td>HH</td>\n",
       "      <td>32.8889</td>\n",
       "      <td>-117.1051</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-11-06 14:50:00</td>\n",
       "      <td>noise</td>\n",
       "      <td>109C.TA_201511061450_NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TA</td>\n",
       "      <td>109C</td>\n",
       "      <td>HH</td>\n",
       "      <td>32.8889</td>\n",
       "      <td>-117.1051</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-11-07 02:20:00</td>\n",
       "      <td>noise</td>\n",
       "      <td>109C.TA_201511070220_NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TA</td>\n",
       "      <td>109C</td>\n",
       "      <td>HH</td>\n",
       "      <td>32.8889</td>\n",
       "      <td>-117.1051</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-11-14 05:15:00</td>\n",
       "      <td>noise</td>\n",
       "      <td>109C.TA_201511140515_NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TA</td>\n",
       "      <td>109C</td>\n",
       "      <td>HH</td>\n",
       "      <td>32.8889</td>\n",
       "      <td>-117.1051</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-12-25 18:50:00</td>\n",
       "      <td>noise</td>\n",
       "      <td>109C.TA_201512251850_NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265652</th>\n",
       "      <td>WY</td>\n",
       "      <td>YHL</td>\n",
       "      <td>HH</td>\n",
       "      <td>44.8509</td>\n",
       "      <td>-111.1830</td>\n",
       "      <td>2691.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>manual</td>\n",
       "      <td>0.68</td>\n",
       "      <td>4.21</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1253</td>\n",
       "      <td>13.93</td>\n",
       "      <td>124.9</td>\n",
       "      <td>[34.40000153 33.         36.70000076]</td>\n",
       "      <td>[[1239.]]</td>\n",
       "      <td>2017-06-21 09:40:43.810000</td>\n",
       "      <td>earthquake_local</td>\n",
       "      <td>YHL.WY_20170621094042_EV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265653</th>\n",
       "      <td>WY</td>\n",
       "      <td>YHL</td>\n",
       "      <td>HH</td>\n",
       "      <td>44.8509</td>\n",
       "      <td>-111.1830</td>\n",
       "      <td>2691.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>manual</td>\n",
       "      <td>0.65</td>\n",
       "      <td>4.84</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1553</td>\n",
       "      <td>17.26</td>\n",
       "      <td>121.7</td>\n",
       "      <td>[44.20000076 43.79999924 41.40000153]</td>\n",
       "      <td>[[1551.]]</td>\n",
       "      <td>2017-06-21 12:21:05.390000</td>\n",
       "      <td>earthquake_local</td>\n",
       "      <td>YHL.WY_20170621122104_EV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265654</th>\n",
       "      <td>WY</td>\n",
       "      <td>YHL</td>\n",
       "      <td>HH</td>\n",
       "      <td>44.8509</td>\n",
       "      <td>-111.1830</td>\n",
       "      <td>2691.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>manual</td>\n",
       "      <td>0.65</td>\n",
       "      <td>4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1534</td>\n",
       "      <td>17.05</td>\n",
       "      <td>121.5</td>\n",
       "      <td>[47.5        39.90000153 42.70000076]</td>\n",
       "      <td>[[1443.]]</td>\n",
       "      <td>2017-06-21 12:23:40.200000</td>\n",
       "      <td>earthquake_local</td>\n",
       "      <td>YHL.WY_20170621122339_EV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265655</th>\n",
       "      <td>WY</td>\n",
       "      <td>YHL</td>\n",
       "      <td>HH</td>\n",
       "      <td>44.8509</td>\n",
       "      <td>-111.1830</td>\n",
       "      <td>2691.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>manual</td>\n",
       "      <td>0.59</td>\n",
       "      <td>4.26</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>14.08</td>\n",
       "      <td>126.6</td>\n",
       "      <td>[24.70000076 21.89999962 21.        ]</td>\n",
       "      <td>[[1322.]]</td>\n",
       "      <td>2017-06-21 13:34:40.330000</td>\n",
       "      <td>earthquake_local</td>\n",
       "      <td>YHL.WY_20170621133439_EV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265656</th>\n",
       "      <td>WY</td>\n",
       "      <td>YHL</td>\n",
       "      <td>HH</td>\n",
       "      <td>44.8509</td>\n",
       "      <td>-111.1830</td>\n",
       "      <td>2691.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>manual</td>\n",
       "      <td>0.59</td>\n",
       "      <td>4.45</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1484</td>\n",
       "      <td>16.50</td>\n",
       "      <td>130.0</td>\n",
       "      <td>[32.90000153 32.79999924 35.59999847]</td>\n",
       "      <td>[[1429.]]</td>\n",
       "      <td>2017-06-21 14:06:11.750000</td>\n",
       "      <td>earthquake_local</td>\n",
       "      <td>YHL.WY_20170621140610_EV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1265657 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        network_code receiver_code receiver_type  receiver_latitude  \\\n",
       "0                 TA          109C            HH            32.8889   \n",
       "1                 TA          109C            HH            32.8889   \n",
       "2                 TA          109C            HH            32.8889   \n",
       "3                 TA          109C            HH            32.8889   \n",
       "4                 TA          109C            HH            32.8889   \n",
       "...              ...           ...           ...                ...   \n",
       "1265652           WY           YHL            HH            44.8509   \n",
       "1265653           WY           YHL            HH            44.8509   \n",
       "1265654           WY           YHL            HH            44.8509   \n",
       "1265655           WY           YHL            HH            44.8509   \n",
       "1265656           WY           YHL            HH            44.8509   \n",
       "\n",
       "         receiver_longitude  receiver_elevation_m  p_arrival_sample p_status  \\\n",
       "0                 -117.1051                 150.0               0.0      NaN   \n",
       "1                 -117.1051                 150.0               0.0      NaN   \n",
       "2                 -117.1051                 150.0               0.0      NaN   \n",
       "3                 -117.1051                 150.0               0.0      NaN   \n",
       "4                 -117.1051                 150.0               0.0      NaN   \n",
       "...                     ...                   ...               ...      ...   \n",
       "1265652           -111.1830                2691.0             400.0   manual   \n",
       "1265653           -111.1830                2691.0             600.0   manual   \n",
       "1265654           -111.1830                2691.0             500.0   manual   \n",
       "1265655           -111.1830                2691.0             500.0   manual   \n",
       "1265656           -111.1830                2691.0             500.0   manual   \n",
       "\n",
       "         p_weight  p_travel_sec  ...  source_magnitude_author  \\\n",
       "0             NaN           NaN  ...                      NaN   \n",
       "1             NaN           NaN  ...                      NaN   \n",
       "2             NaN           NaN  ...                      NaN   \n",
       "3             NaN           NaN  ...                      NaN   \n",
       "4             NaN           NaN  ...                      NaN   \n",
       "...           ...           ...  ...                      ...   \n",
       "1265652      0.68          4.21  ...                      NaN   \n",
       "1265653      0.65          4.84  ...                      NaN   \n",
       "1265654      0.65          4.82  ...                      NaN   \n",
       "1265655      0.59          4.26  ...                      NaN   \n",
       "1265656      0.59          4.45  ...                      NaN   \n",
       "\n",
       "        source_mechanism_strike_dip_rake  source_distance_deg  \\\n",
       "0                                    NaN                  NaN   \n",
       "1                                    NaN                  NaN   \n",
       "2                                    NaN                  NaN   \n",
       "3                                    NaN                  NaN   \n",
       "4                                    NaN                  NaN   \n",
       "...                                  ...                  ...   \n",
       "1265652                              NaN               0.1253   \n",
       "1265653                              NaN               0.1553   \n",
       "1265654                              NaN               0.1534   \n",
       "1265655                              NaN               0.1267   \n",
       "1265656                              NaN               0.1484   \n",
       "\n",
       "        source_distance_km back_azimuth_deg  \\\n",
       "0                      NaN              NaN   \n",
       "1                      NaN              NaN   \n",
       "2                      NaN              NaN   \n",
       "3                      NaN              NaN   \n",
       "4                      NaN              NaN   \n",
       "...                    ...              ...   \n",
       "1265652              13.93            124.9   \n",
       "1265653              17.26            121.7   \n",
       "1265654              17.05            121.5   \n",
       "1265655              14.08            126.6   \n",
       "1265656              16.50            130.0   \n",
       "\n",
       "                                        snr_db  coda_end_sample  \\\n",
       "0                                          NaN              NaN   \n",
       "1                                          NaN              NaN   \n",
       "2                                          NaN              NaN   \n",
       "3                                          NaN              NaN   \n",
       "4                                          NaN              NaN   \n",
       "...                                        ...              ...   \n",
       "1265652  [34.40000153 33.         36.70000076]        [[1239.]]   \n",
       "1265653  [44.20000076 43.79999924 41.40000153]        [[1551.]]   \n",
       "1265654  [47.5        39.90000153 42.70000076]        [[1443.]]   \n",
       "1265655  [24.70000076 21.89999962 21.        ]        [[1322.]]   \n",
       "1265656  [32.90000153 32.79999924 35.59999847]        [[1429.]]   \n",
       "\n",
       "                   trace_start_time    trace_category  \\\n",
       "0               2015-10-21 05:55:00             noise   \n",
       "1               2015-11-06 14:50:00             noise   \n",
       "2               2015-11-07 02:20:00             noise   \n",
       "3               2015-11-14 05:15:00             noise   \n",
       "4               2015-12-25 18:50:00             noise   \n",
       "...                             ...               ...   \n",
       "1265652  2017-06-21 09:40:43.810000  earthquake_local   \n",
       "1265653  2017-06-21 12:21:05.390000  earthquake_local   \n",
       "1265654  2017-06-21 12:23:40.200000  earthquake_local   \n",
       "1265655  2017-06-21 13:34:40.330000  earthquake_local   \n",
       "1265656  2017-06-21 14:06:11.750000  earthquake_local   \n",
       "\n",
       "                       trace_name  \n",
       "0         109C.TA_201510210555_NO  \n",
       "1         109C.TA_201511061450_NO  \n",
       "2         109C.TA_201511070220_NO  \n",
       "3         109C.TA_201511140515_NO  \n",
       "4         109C.TA_201512251850_NO  \n",
       "...                           ...  \n",
       "1265652  YHL.WY_20170621094042_EV  \n",
       "1265653  YHL.WY_20170621122104_EV  \n",
       "1265654  YHL.WY_20170621122339_EV  \n",
       "1265655  YHL.WY_20170621133439_EV  \n",
       "1265656  YHL.WY_20170621140610_EV  \n",
       "\n",
       "[1265657 rows x 35 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting Dataset Paths: Metadata\n",
    "csv_path = 'dataset\\\\stead\\\\metadata.csv'\n",
    "\n",
    "# Read metadata, fill noise data p-arrival value with 0\n",
    "df = pd.read_csv(csv_path)\n",
    "df['p_arrival_sample'] = df['p_arrival_sample'].fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc2493bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:09<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 1265657\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trace_name</th>\n",
       "      <th>p_arrival_sample</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chunk</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[109C.TA_201510210555_NO, 109C.TA_201511061450...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[109C.TA_20060723155859_EV, 109C.TA_2006110315...</td>\n",
       "      <td>[700.0, 600.0, 500.0, 900.0, 700.0, 700.0, 900...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[B087.PB_20111103062804_EV, B087.PB_2011110306...</td>\n",
       "      <td>[599.0, 499.0, 399.0, 900.0, 600.0, 700.0, 800...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[CPE.AZ_20121116014416_EV, CPE.AZ_201211160937...</td>\n",
       "      <td>[400.0, 500.0, 700.0, 1000.0, 400.0, 800.0, 40...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[HPC.NC_20130403221010_EV, HPC.NC_201304040259...</td>\n",
       "      <td>[800.0, 500.0, 600.0, 1000.0, 700.0, 800.0, 90...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[PAH.NN_20151214105520_EV, PAH.NN_201512141336...</td>\n",
       "      <td>[600.0, 400.0, 500.0, 800.0, 400.0, 700.0, 700...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              trace_name  \\\n",
       "chunk                                                      \n",
       "1      [109C.TA_201510210555_NO, 109C.TA_201511061450...   \n",
       "2      [109C.TA_20060723155859_EV, 109C.TA_2006110315...   \n",
       "3      [B087.PB_20111103062804_EV, B087.PB_2011110306...   \n",
       "4      [CPE.AZ_20121116014416_EV, CPE.AZ_201211160937...   \n",
       "5      [HPC.NC_20130403221010_EV, HPC.NC_201304040259...   \n",
       "6      [PAH.NN_20151214105520_EV, PAH.NN_201512141336...   \n",
       "\n",
       "                                        p_arrival_sample  length  \n",
       "chunk                                                             \n",
       "1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    1000  \n",
       "2      [700.0, 600.0, 500.0, 900.0, 700.0, 700.0, 900...    1000  \n",
       "3      [599.0, 499.0, 399.0, 900.0, 600.0, 700.0, 800...    1000  \n",
       "4      [400.0, 500.0, 700.0, 1000.0, 400.0, 800.0, 40...    1000  \n",
       "5      [800.0, 500.0, 600.0, 1000.0, 700.0, 800.0, 90...    1000  \n",
       "6      [600.0, 400.0, 500.0, 800.0, 400.0, 700.0, 700...    1000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting Dataset Paths: Waveform\n",
    "wave_path = lambda x: f'dataset\\\\stead\\\\waveform\\\\chunk{x}.hdf5'\n",
    "\n",
    "counting = 0\n",
    "trace_names = []\n",
    "chuck_name = []\n",
    "\n",
    "# Read Waveform in 6 chunk\n",
    "for i in tqdm(range(6)):\n",
    "    with h5py.File(wave_path(i+1), \"r\") as f:\n",
    "        counting += len(f['data'].keys())\n",
    "        trace_names += list(f['data'].keys())\n",
    "        chuck_name += [i+1] * len(f['data'].keys())\n",
    "        \n",
    "print(\"Total data:\", counting)\n",
    "\n",
    "chunkdict = {trc:chu for trc, chu in zip(trace_names, chuck_name)}\n",
    "df['chunk'] = df['trace_name'].apply(lambda x: chunkdict[x])\n",
    "\n",
    "# Just resample 200000 for each chunk\n",
    "chunk_length = 200000\n",
    "chunk_traces = pd.concat([df[df.chunk==i].iloc[:chunk_length] for i in range(1,7)])[['chunk','trace_name','p_arrival_sample']].groupby('chunk').agg(list)\n",
    "chunk_traces['length'] = chunk_traces['trace_name'].apply(lambda x: len(x))\n",
    "chunk_traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f6594",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cb1f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation function\n",
    "def cstream(dt0, sr1, sr2, data):\n",
    "    # Set the start time, end time, and sampling rate\n",
    "    start_time = UTCDateTime.now() - dt0  # Replace with your desired start time\n",
    "    end_time = UTCDateTime.now()  # Calculate the end time based on the data length and sampling rate\n",
    "    sampling_rate = sr1\n",
    "    st = obspy.Stream()\n",
    "    \n",
    "    # Create an ObsPy trace\n",
    "    trace0 = obspy.Trace(data=data[:,0], header={\n",
    "        'starttime': start_time,\n",
    "        'endtime': end_time,\n",
    "        'sampling_rate': sampling_rate\n",
    "    })\n",
    "    trace1 = obspy.Trace(data=data[:,1], header={\n",
    "        'starttime': start_time,\n",
    "        'endtime': end_time,\n",
    "        'sampling_rate': sampling_rate\n",
    "    })\n",
    "    trace2 = obspy.Trace(data=data[:,2], header={\n",
    "        'starttime': start_time,\n",
    "        'endtime': end_time,\n",
    "        'sampling_rate': sampling_rate\n",
    "    })\n",
    "    st.append(trace0)\n",
    "    st.append(trace1)\n",
    "    st.append(trace2)\n",
    "    trace = st.interpolate(sampling_rate=sr2, method=\"linear\")\n",
    "    return np.moveaxis(np.array([\n",
    "        trace[-3].data[:],\n",
    "        trace[-2].data[:],\n",
    "        trace[-1].data[:],\n",
    "        ]), -2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fe30218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(series, mode):\n",
    "    if mode==\"z_norm\":\n",
    "        mean = np.mean(series)\n",
    "        std = np.std(series)\n",
    "        return (series - mean) / std\n",
    "    elif mode==\"min_max\":\n",
    "        min_val=0\n",
    "        max_val=1\n",
    "        series_min = np.min(series)\n",
    "        series_max = np.max(series)\n",
    "        return (series - series_min) * (max_val - min_val) / (series_max - series_min) + min_val\n",
    "    elif mode==\"mean_norm\":\n",
    "        mean = series.mean(0)\n",
    "        return series - mean\n",
    "    elif mode==\"max_abs\":\n",
    "        max_absolute = np.max(np.abs(series))\n",
    "        return series / max_absolute\n",
    "    elif mode==\"power_transform\":\n",
    "        sign = np.sign(series)\n",
    "        return sign * np.abs(series) ** 0.5\n",
    "    elif mode==\"scaled_norm\": \n",
    "        return series/1000\n",
    "    elif mode==\"proposed_norm\": \n",
    "        return (series - series[0])/1000\n",
    "    return series   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3adfa7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for vectorization data and label\n",
    "def set_label(p_point, window, residual, source_sampling_rate=100, max_data=60):\n",
    "    return 1 if p_point>=(((window/2)-residual)*source_sampling_rate) and p_point<((max_data-(residual+(window/2)))*source_sampling_rate) else 0\n",
    "\n",
    "def get_window(array, label, p_point, window, residual, sampling_rate, source_sampling_rate=100, max_data=60):\n",
    "    # Create available area for wave_start\n",
    "    available = np.arange(0, int((max_data-window)*source_sampling_rate))\n",
    "    # Create wave_start that the P-wave arrival point (p_point) is centered within the window +- residual shift\n",
    "    p_area = np.arange(p_point - (((window/2)+residual)*source_sampling_rate), p_point - (((window/2)-residual)*source_sampling_rate))\n",
    "    if label:\n",
    "        # Random choice wave_start in p-arrival area\n",
    "        wave_start = np.random.choice(p_area)\n",
    "        # Shifting negative value to 0 \n",
    "        wave_start = int(max(wave_start, 0))\n",
    "    else:\n",
    "        # Random choice wave_start excluding p-wave area\n",
    "        wave_start = int(np.random.choice(np.setdiff1d(available, p_area)))\n",
    "    wave = array[wave_start:wave_start+int(window*source_sampling_rate)+1]\n",
    "    if sampling_rate!=source_sampling_rate:\n",
    "        wave = cstream(window, source_sampling_rate, sampling_rate, wave)\n",
    "    return wave[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efbb3ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 3008.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read Waveform in 6 chunk\n",
    "DATA = [0,0,0,0,0,0]\n",
    "for i in tqdm(range(6)):\n",
    "    DATA[i] = h5py.File(wave_path(i+1), \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0b38662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generator\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, data, window, residual, sampling_rate, preprocessing='proposed_norm', batch_size=128):\n",
    "        self.data = data\n",
    "        self.window = window\n",
    "        self.residual = residual\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.preprocessing = preprocessing\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start_index = index * self.batch_size\n",
    "        end_index = min((index + 1) * self.batch_size, len(self.data))\n",
    "        X, y = [], []\n",
    "        balanced_batch = np.random.binomial(n=1, p=0.5, size=self.batch_size)\n",
    "        for enu,idx in enumerate(self.data[start_index:end_index]):\n",
    "            name = chunk_traces.loc[1+idx//chunk_length].trace_name[idx%chunk_length]\n",
    "            data = DATA[idx//chunk_length]['data'][name][:]\n",
    "            p_arrival = chunk_traces.loc[1+idx//chunk_length].p_arrival_sample[idx%chunk_length]\n",
    "            label = chunk_traces.loc[1+idx//chunk_length].label[idx%chunk_length] * balanced_batch[enu]\n",
    "            data = get_window(data, label, p_arrival, self.window, self.residual, self.sampling_rate)\n",
    "            data = prep(data, self.preprocessing)\n",
    "            X.append(data)\n",
    "            y.append(label)\n",
    "        return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6296cfa",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47509c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling function\n",
    "def evaluate_dl(name, clf, train, test, save=False):\n",
    "    start_time = timeit.default_timer()\n",
    "    mymodelbefore = sorted([pt.split('_')[0] for pt in os.listdir(f'{foldername}/{window}s/') if '.h5' in pt])\n",
    "    if name in mymodelbefore:\n",
    "        clf = keras.models.load_model(f'{foldername}/{window}s/{name}_{window}s_{sampling_rate}hz.h5')\n",
    "        clf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        save = False\n",
    "    else:\n",
    "        callbacks = [keras.callbacks.EarlyStopping(patience=50, restore_best_weights=True)]\n",
    "        clf.fit(train, validation_data=test, epochs=500, batch_size=4096, callbacks=callbacks, shuffle=True)\n",
    "    pred_test = clf.predict(test)\n",
    "    loss_test = round(clf.evaluate(test)[0], 5)\n",
    "    acc_test = round(clf.evaluate(test)[1], 5)\n",
    "    elapsed_test = timeit.default_timer()-start_time\n",
    "    print(name)\n",
    "    print('loss_test:',loss_test)\n",
    "    print('acc_test:',acc_test)\n",
    "    print('elapsed:',elapsed_test)\n",
    "    print('==='*50,'\\n')\n",
    "    if save:\n",
    "        clf.save(f'{foldername}/{window}s/{name}_{window}s_{sampling_rate}hz.h5')\n",
    "    return [name, loss_test, acc_test, elapsed_test]  \n",
    "\n",
    "def se_block(input_tensor, ratio=4):\n",
    "    \"\"\"The Squeeze-and-Excitation block.\n",
    "    \n",
    "    Args:\n",
    "    input_tensor: input tensor\n",
    "    ratio: reduction ratio for the squeeze operation\n",
    "    \n",
    "    Returns:\n",
    "    output tensor after the SE block\n",
    "    \"\"\"\n",
    "    # Squeeze operation\n",
    "    res = layers.SeparableConv1D(int(input_tensor.shape[-1]*2), kernel_size=3, padding='same', activation=\"relu\")(input_tensor)\n",
    "    se_avg_pool = layers.GlobalAveragePooling1D()(res)\n",
    "    \n",
    "    se_dense_1 = layers.Dense(input_tensor.shape[-1] // ratio, activation='relu')(se_avg_pool)\n",
    "    se_dense_2 = layers.Dense(res.shape[-1], activation='sigmoid')(se_dense_1)\n",
    "    \n",
    "    # Scale the input\n",
    "    se_scale = layers.Multiply()([res, se_dense_2])\n",
    "    proj = layers.Dense(input_tensor.shape[-1])(se_scale)\n",
    "    proj = layers.Dropout(0.15)(proj)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(proj + input_tensor) \n",
    "\n",
    "# Transformer components\n",
    "def transformer_encoder(inputs, embed_dim, num_heads, ff_dim, rate=0.15):\n",
    "    # Multi-head attention\n",
    "    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
    "    attn_output = layers.Dropout(rate)(attn_output)\n",
    "    out1 = layers.LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ffn_output = layers.Dense(ff_dim, activation=\"linear\")(out1)\n",
    "    ffn_output = keras.layers.LeakyReLU(alpha=0.1)(ffn_output)\n",
    "    ffn_output = layers.Dense(embed_dim)(ffn_output)\n",
    "    ffn_output = layers.Dropout(rate)(ffn_output)\n",
    "    return layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "def convmodel(inputs, spatialdrop=False, se=False):\n",
    "    x = keras.layers.Conv1D(32, kernel_size=3, padding='same', activation=\"relu\")(inputs)\n",
    "    if spatialdrop:\n",
    "        x = keras.layers.SpatialDropout1D(0.15)(x)\n",
    "    x = keras.layers.MaxPooling1D(2)(x)\n",
    "    x = keras.layers.Conv1D(64, kernel_size=3, padding='same', activation=\"relu\")(x)\n",
    "    if spatialdrop:\n",
    "        x = keras.layers.SpatialDropout1D(0.15)(x)\n",
    "    x = keras.layers.MaxPooling1D(2)(x)\n",
    "    x = keras.layers.Conv1D(128, kernel_size=3, padding='same', activation=\"relu\")(x)\n",
    "    if spatialdrop:\n",
    "        x = keras.layers.SpatialDropout1D(0.15)(x)\n",
    "    x = keras.layers.MaxPooling1D(2)(x)\n",
    "    return x\n",
    "    \n",
    "def DLmodel(name, input_shape): \n",
    "    inputs = keras.layers.Input(shape=input_shape)\n",
    "   \n",
    "    if name.split('-')[0]=='01convflat':\n",
    "        x = convmodel(inputs, spatialdrop=False)\n",
    "        layer = keras.layers.Flatten()(x)\n",
    "    if name.split('-')[0]=='02convspat':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        layer = keras.layers.Flatten()(x)\n",
    "    if name.split('-')[0]=='03convse':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        layer = keras.layers.Flatten()(x)\n",
    "    if name.split('-')[0]=='03convse2':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        layer = keras.layers.Flatten()(x)\n",
    "    if name.split('-')[0]=='03convse3':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        layer = keras.layers.Flatten()(x)\n",
    "    \n",
    "    if name.split('-')[0]=='04convset':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        for _ in range(1): x = transformer_encoder(x, 128, 4, 128)\n",
    "        layer = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    if name.split('-')[0]=='05convselstm':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        layer = keras.layers.LSTM(256)(x)\n",
    "    if name.split('-')[0]=='04convse2t':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        for _ in range(1): x = transformer_encoder(x, 128, 4, 128)\n",
    "        layer = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    if name.split('-')[0]=='05convse2lstm':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        layer = keras.layers.LSTM(256)(x)\n",
    "    if name.split('-')[0]=='04convse3t':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        for _ in range(1): x = transformer_encoder(x, 128, 4, 128)\n",
    "        layer = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    if name.split('-')[0]=='05convse3lstm':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        layer = keras.layers.LSTM(256)(x)\n",
    "        \n",
    "    if name.split('-')[0]=='04convset2':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        for _ in range(2): x = transformer_encoder(x, 128, 4, 128)\n",
    "        layer = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    if name.split('-')[0]=='05convselstm2':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        x = keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "        layer = keras.layers.LSTM(256)(x)\n",
    "    if name.split('-')[0]=='04convse2t2':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        for _ in range(2): x = transformer_encoder(x, 128, 4, 128)\n",
    "        layer = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    if name.split('-')[0]=='05convse2lstm2':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        x = keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "        layer = keras.layers.LSTM(256)(x)\n",
    "    if name.split('-')[0]=='04convse3t2':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        for _ in range(2): x = transformer_encoder(x, 128, 4, 128)\n",
    "        layer = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    if name.split('-')[0]=='05convse3lstm2':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        x = keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "        layer = keras.layers.LSTM(256)(x)\n",
    "        \n",
    "    if name.split('-')[0]=='04convset3':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        for _ in range(3): x = transformer_encoder(x, 128, 4, 128)\n",
    "        layer = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    if name.split('-')[0]=='05convselstm3':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        x = keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "        x = keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "        layer = keras.layers.LSTM(256)(x)\n",
    "    if name.split('-')[0]=='04convse2t3':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        for _ in range(3): x = transformer_encoder(x, 128, 4, 128)\n",
    "        layer = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    if name.split('-')[0]=='05convse2lstm3':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        x = keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "        x = keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "        layer = keras.layers.LSTM(256)(x)\n",
    "    if name.split('-')[0]=='04convse3t3':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        for _ in range(3): x = transformer_encoder(x, 128, 4, 128)\n",
    "        layer = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    if name.split('-')[0]=='05convse3lstm3':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        x = se_block(x)\n",
    "        x = keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "        x = keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "        layer = keras.layers.LSTM(256)(x)\n",
    "    \n",
    "    if name.split('-')[0]=='06convt':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        for _ in range(1): x = transformer_encoder(x, 128, 4, 128)\n",
    "        layer = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    if name.split('-')[0]=='07convlstm':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        layer = keras.layers.LSTM(256)(x)\n",
    "    if name.split('-')[0]=='06convt2':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        for _ in range(2): x = transformer_encoder(x, 128, 4, 128)\n",
    "        layer = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    if name.split('-')[0]=='07convlstm2':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "        layer = keras.layers.LSTM(256)(x)\n",
    "    if name.split('-')[0]=='06convt3':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        for _ in range(3): x = transformer_encoder(x, 128, 4, 128)\n",
    "        layer = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    if name.split('-')[0]=='07convlstm3':\n",
    "        x = convmodel(inputs, spatialdrop=True)\n",
    "        x = keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "        x = keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "        layer = keras.layers.LSTM(256)(x)\n",
    "    \n",
    "    if name.split('-')[0]=='08lstm':\n",
    "        layer = keras.layers.LSTM(256)(inputs)\n",
    "    if name.split('-')[0]=='08lstm2':\n",
    "        x = keras.layers.LSTM(256, return_sequences=True)(inputs)\n",
    "        layer = keras.layers.LSTM(256)(x)\n",
    "    if name.split('-')[0]=='08lstm3':\n",
    "        x = keras.layers.LSTM(256, return_sequences=True)(inputs)\n",
    "        x = keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "        layer = keras.layers.LSTM(256)(x)\n",
    "    if name.split('-')[0]=='09trans':\n",
    "        x = inputs\n",
    "        for _ in range(1): x = transformer_encoder(x, 3, 4, 128)\n",
    "        layer = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    if name.split('-')[0]=='09trans2':\n",
    "        x = inputs\n",
    "        for _ in range(2): x = transformer_encoder(x, 3, 4, 128)\n",
    "        layer = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    if name.split('-')[0]=='09trans3':\n",
    "        x = inputs\n",
    "        for _ in range(3): x = transformer_encoder(x, 3, 4, 128)\n",
    "        layer = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    \n",
    "    if name.split('-')[0]=='08lstmspat':\n",
    "        x = keras.layers.LSTM(256, return_sequences=True)(inputs)\n",
    "        x = keras.layers.SpatialDropout1D(0.15)(x)\n",
    "        layer = keras.layers.LSTM(256)(x)\n",
    "    if name.split('-')[0]=='09transspat':\n",
    "        x = inputs\n",
    "        x = transformer_encoder(x, 3, 4, 128)\n",
    "        x = keras.layers.SpatialDropout1D(0.15)(x)\n",
    "        x = transformer_encoder(x, 3, 4, 128)\n",
    "        layer = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    if name.split('-')[0]=='03convsenospat':\n",
    "        x = convmodel(inputs, spatialdrop=False)\n",
    "        x = se_block(x)\n",
    "        layer = keras.layers.Flatten()(x)\n",
    "    if name.split('-')[0]=='04convsetnospat':\n",
    "        x = convmodel(inputs, spatialdrop=False)\n",
    "        x = se_block(x)\n",
    "        for _ in range(1): x = transformer_encoder(x, 128, 4, 128)\n",
    "        layer = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    if name.split('-')[0]=='05convselstmnospat':\n",
    "        x = convmodel(inputs, spatialdrop=False)\n",
    "        x = se_block(x)\n",
    "        layer = keras.layers.LSTM(256)(x)\n",
    "    if name.split('-')[0]=='06convtnospat':\n",
    "        x = convmodel(inputs, spatialdrop=False)\n",
    "        for _ in range(1): x = transformer_encoder(x, 128, 4, 128)\n",
    "        layer = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    if name.split('-')[0]=='07convlstmnospat':\n",
    "        x = convmodel(inputs, spatialdrop=False)\n",
    "        layer = keras.layers.LSTM(256)(x)\n",
    "\n",
    "    layer = keras.layers.Dense(64, activation='relu')(layer)\n",
    "    layer = keras.layers.Dropout(0.5)(layer)\n",
    "    outputs = keras.layers.Dense(1, activation='sigmoid')(layer)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f267f81",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b09a7aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "window = 8\n",
    "sampling_rate = 20\n",
    "residual = 1\n",
    "batch_size = 128\n",
    "\n",
    "# Set label for each chunk\n",
    "label_chunk = [[set_label(v, window, residual) for v in chunk_traces.loc[i]['p_arrival_sample']] for i in range(1,7)]\n",
    "chunk_traces['label'] = label_chunk\n",
    "\n",
    "# Split data into training-validation-testing\n",
    "train_series = pd.DataFrame({'label':label_chunk[0]+label_chunk[1]+label_chunk[2]+label_chunk[3]})\n",
    "valid_series = pd.DataFrame({'label':label_chunk[4]+label_chunk[5]})\n",
    "train_series = train_series.sample(train_series.shape[0])\n",
    "valid_series = valid_series.sample(valid_series.shape[0])\n",
    "\n",
    "# Get data with generator\n",
    "train_gen = DataGenerator(train_series.index.values, window, residual, sampling_rate, preprocessing='proposed_norm', batch_size=batch_size)\n",
    "valid_gen = DataGenerator(valid_series.index.values, window, residual, sampling_rate, preprocessing='proposed_norm', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e1d0c5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"04convset\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 160, 3)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 160, 32)      320         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_18 (SpatialDr (None, 160, 32)      0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 80, 32)       0           spatial_dropout1d_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 80, 64)       6208        max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_19 (SpatialDr (None, 80, 64)       0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 40, 64)       0           spatial_dropout1d_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 40, 128)      24704       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_20 (SpatialDr (None, 40, 128)      0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 20, 128)      0           spatial_dropout1d_20[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv1d_6 (SeparableCo (None, 20, 256)      33408       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 256)          0           separable_conv1d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 32)           8224        global_average_pooling1d_12[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_43 (Dense)                (None, 256)          8448        dense_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 20, 256)      0           separable_conv1d_6[0][0]         \n",
      "                                                                 dense_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_44 (Dense)                (None, 20, 128)      32896       multiply_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 20, 128)      0           dense_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_18 (TFOpLa (None, 20, 128)      0           dropout_26[0][0]                 \n",
      "                                                                 max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_18 (LayerNo (None, 20, 128)      256         tf.__operators__.add_18[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_6 (MultiHe (None, 20, 128)      263808      layer_normalization_18[0][0]     \n",
      "                                                                 layer_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 20, 128)      0           multi_head_attention_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_19 (TFOpLa (None, 20, 128)      0           layer_normalization_18[0][0]     \n",
      "                                                                 dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_19 (LayerNo (None, 20, 128)      256         tf.__operators__.add_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_45 (Dense)                (None, 20, 128)      16512       layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 20, 128)      0           dense_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_46 (Dense)                (None, 20, 128)      16512       leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 20, 128)      0           dense_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_20 (TFOpLa (None, 20, 128)      0           layer_normalization_19[0][0]     \n",
      "                                                                 dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_20 (LayerNo (None, 20, 128)      256         tf.__operators__.add_20[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 20)           0           layer_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_47 (Dense)                (None, 64)           1344        global_average_pooling1d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 64)           0           dense_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 1)            65          dropout_29[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 413,217\n",
      "Trainable params: 413,217\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "32/32 [==============================] - 18s 491ms/step - loss: 0.6725 - accuracy: 0.6302 - val_loss: 0.5840 - val_accuracy: 0.7560\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 15s 469ms/step - loss: 0.5794 - accuracy: 0.6520 - val_loss: 0.4027 - val_accuracy: 0.9175\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 15s 464ms/step - loss: 0.3955 - accuracy: 0.8860 - val_loss: 0.2054 - val_accuracy: 0.9500\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 15s 465ms/step - loss: 0.2551 - accuracy: 0.9145 - val_loss: 0.1768 - val_accuracy: 0.9450\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 15s 460ms/step - loss: 0.1924 - accuracy: 0.9355 - val_loss: 0.1304 - val_accuracy: 0.9525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 5s 305ms/step - loss: 0.1369 - accuracy: 0.9500\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.1194 - accuracy: 0.9595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:34<00:00, 94.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04convset\n",
      "loss_test: 0.13689\n",
      "acc_test: 0.9595\n",
      "elapsed: 94.37375909999992\n",
      "====================================================================================================================================================== \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create folder\n",
    "# Create folder\n",
    "foldername = \"save_model/stead\"\n",
    "try:\n",
    "    os.makedirs(foldername+'/'+str(window)+'s')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "result = []\n",
    "modelnames = [\n",
    "     '04convset', #'08lstmspat', '09transspat', '03convsenospat', '05convselstmnospat', '06convtnospat', '07convlstmnospat',\n",
    "    ]\n",
    "for name in tqdm(modelnames):\n",
    "    res = evaluate_dl(name, DLmodel(name, (int(window*sampling_rate), 3)), train_gen, valid_gen, save=True)\n",
    "    result.append(res)\n",
    "    df_result = pd.DataFrame(result, columns=['model', 'loss', 'accuracy', 'elapsed'])\n",
    "    df_result.to_csv(f'{foldername}/{window}s/result_{name}_{window}s_{sampling_rate}hz.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b56848e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
